{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AGM_gyC8F14"
   },
   "source": [
    "# Bioinformatics Vignette: Using K-Nearest Neighbors and Binary Decision Tree Algorithms to Predict Enzyme Function from Protein Sequences\n",
    "\n",
    "## *Vignette Author*: Ayomikun Akinrinade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jv3Ykz8eBtwN"
   },
   "source": [
    "<img src=\"input/ayomikun_akinrinade_vignette_picture.jpeg\" width=\"200\" align=\"left\" style=\"margin: 0px 10px 0px 0px;\" description=\"A picture of Ayomikun Akinrinade, the author of this vignette.\">\n",
    "\n",
    "## About Me\n",
    "My interests primarily lie in evolutionary biology, specifically the morphological evolution of fishes (the most diverse group of vertebrates on earth!). I have been interested in programming for at least ten years, as of writing this vignette, and in discovering my passion for biology, I've developed a curiosity for how we can apply computer science methods to biology. In this vignette, I've shown one way that we can use machine learning to help find solutions in biology. I hope to apply similar techniques to my own research in the future. I similarly hope that you, the reader, will take the knowledge from this chapter and vignette, expand on it, and create a project that you find interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itDY5xtpEMnk"
   },
   "source": [
    "## Background\n",
    "\n",
    "Proteins are responsible for carrying out most enzymatic activities in cells. As such, proteins are a subject frequently under the (metaphorical) microscope of bioinformaticians. Applications range from predicting protein structure (e.g., AlphaFold developed by Google's DeepMind) to predicting protein function. This vignette will focus on latter of these two. *Note however, that protein function is determined by protein structure and shape.*\n",
    "\n",
    "\n",
    "In this vignette, we will be focusing on using amino acid sequences to classify proteins according to their enzymatic function. We will do this using data from the Research Collaboratory for Structural Bioinformatics' Protein Data Base (PDB). From the PDB, we have both the protein sequence data and the known function. We need the sequence data so we can create features that our classfication algorithms will be able to interpret, and we need the known function in order for the algorithm to learn how to predict function from a new sequence. We are going to focus on 6 major enzyme classes: hydrolase, transferase, oxidoreductase, lyase, isomerase, ligase. \n",
    "\n",
    "We will be comparing two different machine learning algorithms: 1) a k-nearest neighbors classifier, and 2) a binary decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### Import Libraries\n",
    "\n",
    "First we'll import the libraries we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "1GZgh5_YLyUK",
    "outputId": "a42e1217-deaf-4b4b-e3f3-61fe1093aa9e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from zipfile import ZipFile\n",
    "from os import listdir\n",
    "\n",
    "np.random.seed(7) # this is so we have consistent results between users; feel free to change this number to see what happens to our results at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip and Import Sequence and Function Data\n",
    "\n",
    "Next, we load protein sequence and function data as pandas DataFrames. \n",
    "\n",
    "One small difficulty is that the sequence data file is 120MB, which is too large to store on GitHub directly. Therefore I had to upload it as a compressed .zip file. We can first uncompress it, to create a `pdb_data_seq.csv` file, and then load that `.csv` file as a pandas DataFrame as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "1GZgh5_YLyUK",
    "outputId": "a42e1217-deaf-4b4b-e3f3-61fe1093aa9e"
   },
   "outputs": [],
   "source": [
    "#Extract the zip file into a text file of sequence data\n",
    "zipfile = ZipFile('input/pdb_data_seq.zip')\n",
    "zipfile.extract(member = 'input/pdb_data_seq.csv', path='./')\n",
    "\n",
    "#Check that the unzipped file is in the input directory\n",
    "print(\"We have these files in our input directory:\",listdir('./input'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "1GZgh5_YLyUK",
    "outputId": "a42e1217-deaf-4b4b-e3f3-61fe1093aa9e"
   },
   "outputs": [],
   "source": [
    "#Load the sequence data as a pandas dataframe\n",
    "all_seqs_df = pd.read_csv('input/pdb_data_seq.csv')\n",
    "\n",
    "#Load the protein function data as a pandas dataframe\n",
    "all_charcs_df = pd.read_csv('input/pdb_data_no_dups.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the data\n",
    "\n",
    "Next, we'll take a look at each dataframe to make sure we understand the form of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the first 5 rows of the sequence dataframe\n",
    "all_seqs_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the first 5 rows of the function dataframe\n",
    "all_charcs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both the sequence and function dataframes up above, there are both proteins and other macromolecules like DNA or DNA/RNA hybrids. We'll need to filter our data to just the protein sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit the data to proteins\n",
    "\n",
    "We saw that PDB data contains data on sturctures other than proteins, but we don't want those in our analysis. The code below drops all structures that aren't proteins from the sequence data and the function classification data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJWVz6SvRNqe"
   },
   "outputs": [],
   "source": [
    "protein_seqs = all_seqs_df[all_seqs_df.macromoleculeType == 'Protein'].reset_index(drop=True) \n",
    "protein_charcs = all_charcs_df[all_charcs_df.macromoleculeType == 'Protein'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and Standardize the Data\n",
    "\n",
    "We want the data all in a single dataframe. We'll do this using the dataframe `.join()` method. We also want to drop proteins with unknown function, and to ensure all the proteins have consistent capitalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_seqs = protein_seqs[['structureId','sequence']]\n",
    "protein_charcs = protein_charcs[['structureId', 'classification']]\n",
    "\n",
    "# combine potein characteristics df with their sequences using structureId\n",
    "protein_all = protein_charcs.set_index('structureId').join(protein_seqs.set_index('structureId'))\n",
    "protein_all = protein_all.dropna()\n",
    "\n",
    "# capitalize all classification values to avoid missing any values in the next step\n",
    "protein_all.classification = protein_all.classification.str.upper()\n",
    "\n",
    "# drop all proteins with an unknown function; note -- the tilde ~ returns the inverse of a filter\n",
    "data = protein_all[~protein_all.classification.str.contains(\"UNKNOWN FUNCTION\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paring down our sequences\n",
    "In the following code, we drop all types of proteins that don't have more than 800 instances in the PDB. We also drop all of the duplicates of structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_count = protein_all.classification.value_counts()\n",
    "functions = np.asarray(class_count[(class_count > 800)].index)\n",
    "data = protein_all[protein_all.classification.isin(functions)]\n",
    "data = data.drop_duplicates(subset=[\"classification\",\"sequence\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop all sequences that aren't enzymes\n",
    "\n",
    "Notice in the head of our data, we see proteins that aren't enzymes (e.g., oxygen transport proteins). So we need to remove these in order to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[~data['classification'].str.contains('ASE'), 'classification'] = 'OTHER'\n",
    "data = data.loc[~data['classification'].str.contains(\"OTHER\")]\n",
    "data.loc[data['classification'].str.contains('TRANSFERASE/TRANSFERASE INHIBITOR'), 'classification'] = 'TRANSFERASE'\n",
    "data.loc[data['classification'].str.contains('OXIDOREDUCTASE/OXIDOREDUCTASE INHIBITOR'), 'classification'] = 'OXIDOREDUCTASE'\n",
    "data.loc[data['classification'].str.contains('HYDROLASE/HYDROLASE INHIBITOR'), 'classification'] = 'HYDROLASE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display how many of each type of enzyme class we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.classification.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at what our data now looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Creation\n",
    "\n",
    "We are going to use a machine learning workflow to classify protein function from sequence, similar to having a classifier try to predict the name of a fruit from its properties. In order for a machine learning algorithm to classify our proteins, we need to create features that the computer can recognize. The same way, in order for us to classify different fruits, we would need features like color, shape, and size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amino Acid Composition\n",
    "\n",
    "We will use the Amino Acid composition of protein sequences as the data for our machine learning classifiers. To do this, we basically have to calculate the percentage of each amino acid in each protein sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create function to convert amino acid sequence string into a list of percentages\n",
    "\n",
    "This next section of code defines a function that:\n",
    "1. creates a list of the amino acids\n",
    "2. makes a function that convert the amino acid sequence, seen in our previous step, into a list of percentages (e.g., [0.02, 0.13, 0.08, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amino_acid_composition(seq):\n",
    "    \"\"\"\n",
    "    Returns a list of the percentages of each amino acid.\n",
    "    seq -- a string representing an amino acid sequence\n",
    "    \"\"\"\n",
    "    aa_list = ['A','R','N','D','C','Q','E','G','H','I','L','K','M','F','P','S','T','W','Y','V']\n",
    "    aac = []\n",
    "    for i in aa_list:\n",
    "        aac.append(seq.count(i)/len(seq))\n",
    "    return aac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a new dataframe of the amino acid compositions, and one for enzyme classes\n",
    "This next step uses the `amino_acid_composition()` function on the enzyme sequences, and stores the result in the AAC column (**AAC** stands for **A**mino **A**acid **C**omposition). After that, we drop the sequences because we now have the amino acid compositions in their place. We then keep the enzyme classes in a seperate dataframe for the Classification step later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['AAC'] = data['sequence'].apply(amino_acid_composition)\n",
    "data.drop(columns='sequence', inplace=True)\n",
    "data_classes = data['classification']\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expand list of amino acid compositions into columns\n",
    "The next two sections of code are going to convert our AAC column in the previous step into a series of columns, each with the heading being each amino acid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_amino_acid_dictionary(percentage_list):\n",
    "    \"\"\"\n",
    "    Returns a dictionary with the keys being the list of amino acids (aa_list) and the value being the percentage\n",
    "    list -- list of amino acids composition percentages\n",
    "    \"\"\"\n",
    "    aa_list = ['A','R','N','D','C','Q','E','G','H','I','L','K','M','F','P','S','T','W','Y','V']    \n",
    "    return dict(zip(aa_list,percentage_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dictAAC'] = data.AAC.apply(build_amino_acid_dictionary)\n",
    "data = data.dictAAC.apply(pd.Series)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting our data into training data and test data\n",
    "The train_test_split() function seperates our data in training data and test data.\n",
    "\n",
    "In order to be able to tell whether our method is working, we want to split up the protein sequences into two sets. The first will be the 'training set' on which the machine learning classifiers learn to tell which protein functions go with which features of amino acid composition. The second will be the 'test set' which *wasn't* used in training. We'll use the test set to test the accuracy of our classifiers.\n",
    "\n",
    "Going back to the fruit example mentioned earlier -- if we want to classify a fruit we haven't seen before, we need to be trained on what different fruits look like.\n",
    "\n",
    "For example: if I place an apple and an orange in front of a toddler, the toddler will have no idea what kind of fruit it is. But, if I expose that toddler to different fruits, explaining what makes an apple an apple, what makes an orange an orange, etc., they will be able to classify whether any fruit I place in front of it is an apple, an orange, etc. That is similar to our training data.\n",
    "\n",
    "In addition, the training data and test data need to be different in order to test if our classifiers are truly predicting the function.\n",
    "\n",
    "Again, going back to the fruit example: if we show that same toddler a red apple they have already seen before, and ask them if that exact same piece of fruit is an apple, we are simply testing memory and not whether it can infer whether any given fruit is an apple or orange. On the other hand if we show them a different type of apple (maybe green rather than red), we can really see if they get the idea of what makes an apple an apple.\n",
    "\n",
    "Helpfully, scikit-learn already has a function for splitting up training and test data called `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into training and test sets\n",
    "X = data\n",
    "y = data_classes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test our Classifiers\n",
    "\n",
    "Now that we have training and test datasets with known amino acid compositions and enzymatic functions, we can use them to train and then test any machine learning algorithms we'd like to try out. In this vignette we'll use two approaches: a k-nearest neighbors classifier and a binary decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Explanation\n",
    "\n",
    "A k-nearest neighbors classifier is a supervised classification algorithm that plots your on axes of a given number of dimensions. The amount of dimensions depends on how many dimensions your input data is in (e.g., input data with two traits to classify on, will create k-nearest neighbors classifier that plots data in two dimensions). Displayed below is an example to illustrate how a k-nearest neighbors classifier works. Here we have two dimensional data and the purple dot in the center is the point we are trying to classify. Surrounding our point are points with known classification. A k-nearest neighbors classifier will count how many of each class is in the k-nearest neighbors and make a decision based on which class makes up the majority. We can see that when *k*=1, we would decide that our point is in Class B. When *k*=3, Class A, and when *k*=5, Class B again. \n",
    "\n",
    "Note: when we are choosing our number for *k*, we never choose an even number because if there are an equal number of Class A and Class B in our sample, we can end up with an equal number of each, which doesn't allow the classifier to make a decision.\n",
    "\n",
    "\n",
    "<img src=\"input/knnexample.png\" width=600px align=\"center\" description=\"A vertebrate binary decision tree\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn_predictions = knn.predict(X_test)\n",
    "knn_score = accuracy_score(y_test, knn_predictions)\n",
    "print(f'Classification accuracy (kNN): {knn_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Explanation\n",
    "\n",
    "A binary decision tree is another supervised classification algorithm that makes a \"tree\" of questions that help the algorithm make decision on what it is trying to categorize. Displayed below is an example to illustrate how a binary decision tree could be used to classify some drinks. The complexity of a binary decision tree can be explained by the number of nodes (or questions) that are needed to classify any given object. In this case simple example, the number of nodes is 2. The next section of code will call the binary decision tree classifier, learn from our training data, predict the classification of the test data and and output the accuracy of the predictions.\n",
    "\n",
    "For further explanation, Michael Galarnyk from Towards Data Science has a great explanation of binary decision trees [here](https://towardsdatascience.com/understanding-decision-trees-for-classification-python-9663d683c952).\n",
    "\n",
    "<img src=\"input/bdtexample.png\" width=800px align=\"center\" description=\"A vertebrate binary decision tree\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt = tree.DecisionTreeClassifier()\n",
    "bdt.fit(X_train, y_train)\n",
    "\n",
    "bdt_predictions = bdt.predict(X_test)\n",
    "bdt_score = accuracy_score(y_test, bdt_predictions)\n",
    "print(f'Classification accuracy (BDT): {bdt_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a graph of the scores for each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dictionary = {'KNN': knn_score, 'BDT': bdt_score}\n",
    "keys = accuracy_dictionary.keys()\n",
    "values = accuracy_dictionary.values()\n",
    "magenta = \"#ff13a8\"\n",
    "orange = \"#fdad01\"\n",
    "plt.bar(keys, values, color=[magenta, orange])\n",
    "plt.xlabel('Classifier Type')\n",
    "plt.ylabel('Accuracy Score (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in accuracy_dictionary.items():\n",
    "    print(f'Accuracy of {k} is ~{round(v*100,1)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the k-nearest neighbors classifer is a better predictor of enzyme function than a binary decision tree in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Try removing the \"np.random.seed(7)\" line from the code and see what results you get for the classification steps\n",
    "\n",
    "2. Try changing the number of neighbors used in the k-Nearest Neighbors Classifier. Does the accuracy change?\n",
    "\n",
    "3. Use a for loop to automate trying out different numbers of neighbors with k-Nearest Neighbors, and select the best one based on its test accuracy.\n",
    "\n",
    "4. The bdt.tree_.node_count property of the binary decision tree says how many nodes were needed for the classifier to make a decision. Try using it to find this out after the binary decision tree classifier has been made.\n",
    "\n",
    "5. The [scikit-learn library](https://scikit-learn.org/stable/) has many other classifiers besides k-nearest neighbors and binary decision trees. Research one of them and try applying it to our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Reading Responses & Feedback](https://docs.google.com/forms/d/e/1FAIpQLSeUQPI_JbyKcX1juAFLt5z1CLzC2vTqaCYySUAYCNElNwZqqQ/viewform?usp=pp_url&entry.2118603224=Bioinformatics+Vignette+-+Ayomikun+Akinrinade+-+Using+k-Nearest+Neighbor+and+Binary+Decision+Tree+Algorithms+to+Predict+Enzyme+Function+from+Protein+Sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- The scikit-learn documentation is very useful for finding example machine learning methods\n",
    "https://scikit-learn.org/stable/\n",
    "\n",
    "- You can read the original description of the protein data bank (PDB) here:\n",
    "\n",
    "Helen M. Berman, John Westbrook, Zukang Feng, Gary Gilliland, T. N. Bhat, Helge Weissig, Ilya N. Shindyalov, Philip E. Bourne, The Protein Data Bank, Nucleic Acids Research, Volume 28, Issue 1, 1 January 2000, Pages 235–242, https://doi.org/10.1093/nar/28.1.235"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "Thanks to Shahir Kottilingal (https://www.shahirk.com/) for compiling the dataset used for this vignette, which can be found at: https://www.kaggle.com/shahir/protein-data-set"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
